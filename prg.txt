Sky,AirTemp,Humidity,Wind,WaterForecast,Forecast,Sport
Sunny,Warm,Normal,Strong,Warm,Same,Yes
Sunny,Warm,High,Strong,Warm,Same,Yes
Rainy,Cold,High,Strong,Warm,Change,No
Sunny,Warm,High,Strong,Cool,Change,Yes

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA

# Load Iris dataset
data = load_iris()
X = data.data
y = data.target
labels = data.target_names
colors = ['r', 'g', 'b']

# --- 3D Plot (first 3 features) ---
fig = plt.figure()
ax = plt.axes(projection='3d')
for i in np.unique(y):
    ax.scatter(X[y == i, 0], X[y == i, 1], X[y == i, 2], color=colors[i], label=labels[i])
ax.set_title('3D: Original Features')
ax.legend()
plt.show()

# --- 2D PCA Plot ---
X_pca = PCA(n_components=2).fit_transform(X)

plt.figure()
for i in np.unique(y):
    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], color=colors[i], label=labels[i])
plt.title('2D: After PCA')
plt.legend()
plt.grid(True)
plt.show()


------------------------------

import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

np.random.seed(42)
vals = np.random.rand(100)
labels = ['Class1' if v <= 0.5 else 'Class2' for v in vals[:50]] + [None]*50
df = pd.DataFrame({'Value': vals, 'Label': labels})

train, test = df[:50], df[50:]
print(test.head())
print(train.head())
true = ['Class1' if v <= 0.5 else 'Class2' for v in test['Value']]

for k in [1, 2, 3, 4, 5, 20, 30]:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(train[['Value']], train['Label'])
    pred = knn.predict(test[['Value']])
    print(f"k={k} Acc: {accuracy_score(true, pred)*100}%")
    test[f'P{k}'] = pred

print(test.head())


---------------------------------------
from sklearn.datasets import fetch_olivetti_faces
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Load data
data = fetch_olivetti_faces()
X, y, images = data.data, data.target, data.images

# Split data
X_train, X_test, y_train, y_test, img_train, img_test = train_test_split(
    X, y, images, test_size=0.2, random_state=42)

# Train model
model = GaussianNB()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Calculate results
total = len(y_test)
wrong = sum(y_test != y_pred)
acc = model.score(X_test,y_test)

print("Number of misclassified images:", wrong)
print("Total images in test set:", total)
print("Accuracy",acc)

# Show some test images with predictions
plt.figure(figsize=(10, 6))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.imshow(img_test[i], cmap='gray')
    plt.title(f"Pred: {y_pred[i]}\nTrue: {y_test[i]}")
plt.suptitle("Face Predictions with Naive Bayes")
plt.tight_layout()
plt.show()


---------------------------

from sklearn.datasets import load_breast_cancer
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Load data and class names
data = load_breast_cancer()
X, y = data.data, data.target
class_names = data.target_names

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Train model
model = DecisionTreeClassifier(max_depth=3)
model.fit(X_train, y_train)

# Accuracy
acc = model.score(X_test, y_test)
print("Accuracy:", acc)

# Predict first test sample and map to class name
pred_class = model.predict([X_test[0]])[0]
print("Predicted class:", class_names[pred_class])

# Plot tree
plt.figure(figsize=(12, 6))
plot_tree(model, feature_names=data.feature_names, class_names=class_names, filled=True)
plt.show()


-----------------------------

#lab-10
from sklearn.datasets import load_breast_cancer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load dataset and apply KMeans
data = load_breast_cancer()
X = data.data

kmeans = KMeans(n_clusters=2, random_state=42)
labels = kmeans.fit_predict(X)

# Reduce dimensions with PCA for visualization
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
centroids_pca = pca.transform(kmeans.cluster_centers_)

# Plot clusters and centroids
plt.figure(figsize=(8, 5))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', s=30)
plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], c='red', marker='X', s=200, label='Centroids')
plt.title("K-Means Clustering (PCA Projection)")
plt.xlabel("PCA 1")
plt.ylabel("PCA 2")
plt.legend()
plt.show()


------------------------------------------

#find s-4,
import pandas as pd

# Load dataset
df = pd.read_csv(r"C:\Users\aniket\Desktop\play.csv")

# Start with first positive example
for i in range(len(df)):
    if df.iloc[i, -1].lower() == 'yes':
        hypo = df.iloc[i, :-1].tolist()
        break

# Update hypothesis
for i in range(len(df)):
    if df.iloc[i, -1].lower() == 'yes':
        for j in range(len(hypo)):
            if hypo[j] != df.iloc[i, j]:
                hypo[j] = '?'

print("Most Specific Hypothesis:", hypo)


---------------------------------
# Lab 1 - Simple Data Check and Plots

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_california_housing

# Load dataset
data = fetch_california_housing(as_frame=True)
df = data.frame

# Basic info
print(df.head())             # Show first 5 rows
print(df.shape)              # Rows and columns
print(df.isnull().sum())     # Missing values
print(df.duplicated().sum()) # Duplicate rows
print(df.describe())         # Summary stats

# Plot histograms
for col in df.select_dtypes('number'):
    sns.histplot(df[col], bins=30, kde=True)
    plt.title(col)
    plt.show()

# Plot boxplots
for col in df.select_dtypes('number'):
    sns.boxplot(x=df[col])
    plt.title(col)
    plt.show()


-------------------------------

# lab2 more easy version-
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing

# Load dataset
data = fetch_california_housing()
df = pd.DataFrame(data.data, columns=data.feature_names)
df['Target'] = data.target

# 1. Correlation matrix
print("\nCorrelation Matrix:")
print(df.corr())

# 2. Heatmap
sns.heatmap(df.corr(), annot=True)
plt.title("Correlation Heatmap")
plt.show()

# 3. Pairplot (only key features to keep it clean)
sns.pairplot(df[['MedInc', 'HouseAge', 'AveRooms', 'Target']])
plt.show()

---------------------------------------------------------------------------------------
import numpy as np
import matplotlib.pyplot as plt
from numpy.polynomial.polynomial import Polynomial

# Generate data
x = np.linspace(1, 10, 100)
y = np.sin(x) + np.random.normal(0, 0.1, 100)

# LWR function
def lwr(x0, x, y, tau=0.5):
    w = np.exp(-((x - x0) ** 2) / (2 * tau ** 2))
    X_ = np.c_[np.ones_like(x), x]
    x0_ = np.array([1, x0])
    W = np.diag(w)
    theta = np.linalg.pinv(X_.T @ W @ X_) @ X_.T @ W @ y
    return x0_ @ theta

# Predict using LWR
y_pred_lwr = [lwr(xi, x, y) for xi in x]

# Linear Regression
X_linear = np.c_[np.ones_like(x), x]
theta_linear = np.linalg.pinv(X_linear.T @ X_linear) @ X_linear.T @ y
y_pred_linear = X_linear @ theta_linear

# Polynomial Regression (degree 3)
poly = Polynomial.fit(x, y, 3)
y_pred_poly = poly(x)

# LWR plot
plt.figure(figsize=(6, 4))
plt.scatter(x, y, color='lightblue')
plt.plot(x, y_pred_lwr, color='red')
plt.title("Locally Weighted Regression")
plt.show()

plt.figure(figsize=(6, 4))
plt.scatter(x, y, color='lightblue')
plt.plot(x, y_pred_linear, color='green')
plt.title("Linear Regression")
plt.show()

plt.figure(figsize=(6, 4))
plt.scatter(x, y, color='lightblue')
plt.plot(x, y_pred_poly, color='purple')
plt.title("Polynomial Regression")
plt.show()
